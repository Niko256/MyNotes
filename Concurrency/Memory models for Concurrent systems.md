
## The End of the Single-threaded fairy tale.

### The `as-if` rule.

В мире однопоточной среды исполнения, разработчики десятилетиями жили под защитой простого соглашения, известного как **принцип `as-if`**.

Этот принцип достаточно краток и понятен : компилятор языка программирования и процессор имеют право производить любые преобразования и оптимизации программы : менять инструкции местами, удалять избыточные инструкции, кэшировать значения в регистрах - до тех пор, пока наблюдаемое поведение программы в рамках одного потока остаётся таким же, как если бы она исполнялась в точности так, как было написано в её исходном коде.

Для нас это был идеальный мир, в котором можно было писать простой, последовательный код, а компилятор и новое поколение процессоров брали на себя магию его оптимизации и ускорения. Детали этих преобразований оставались скрытыми от нас. Главное, что конечный результат был корректным и предсказуемым.

### The multi-core revolution

Эта модель, в которой оптимизации были невидимым благом, закончилась с наступлением эры многоядерных процессоров. Появление множества ядер, а с ними и потоков исполнения, добавило в систему *сторонних наблюдателей*. И эти наблюдатели смогли увидеть то, что раньше было скрыто внутри принципа `as-if`, а именно внутренние трюки компилятора и процессора. Оптимизации, абсолютно безопасные для однопоточной среды, внезапно стали приводить к видимым и зачастую катастрофическим последствиям.

Рассмотрим классический пример, демонстрирующий суть проблемы. Предположим, что мы в одном потоке подготавливаем какой-то ресурс, а затем выставляем флаг, сигнализируя второму потоку, что ресурс готов.

``` C++
int x = 0;
bool done = false;

/* Thread#1 */
x = 42;

/* Thread#2 */
while (!done) {
	done = true;
}

std::cout << x << std::endl; // expecting 42 here...
```

Наше намерение здесь чисто: второй поток должен увидеть `x = 42`. В однопоточной среде порядок `x = 42` и `done = true` был бы незыблем. Но с точки зрения компилятора, оптимизирующего первый поток, переменные `x` и `done` никак не связаны между собой. Он может счесть для себя более эффективным, что сначала эффективней будет выполнить `done = true`, а затем уже `x = 42` (например для более оптимального использования регистров). Такое переупорядочивание само по себе не нарушает логику первого потока, но для второго потока оно фатально : он *может* прочитать `done = true`, выйти из цикла и прочитать из `x` старое значение `0`.

И даже если компилятор сгенерирует инструкции в правильном порядке, то нам также не стоит забывать, что многоядерные процессоры - это невероятно сложные системы с аппаратной точки зрения и они также являются авторами проблем в конкурентной среде. Далее мы разберём проблему детальнее, но вообще говоря, процессорное ядро, выполнив запись `x = 42`, скорее всего, отложит эту запись в специальный *буфер*, чтобы не простаивать при следующем обращении. А вот следующая запись `done = true` может быть выполнена и стать видимой для остальных ядер быстрее, чем первая запись. Снова получаем тот же результат : второй поток видит флаг, но не видит *вроде-как* записанного ресурса.

### So, what is a MM?

Именно этот конфликт между нашими желаниями предсказуемого порядка выполнения и стремлением системы к производительности, привёл человечество к необходимости формализации нового контракта. Этот контракт и есть **Модель памяти (`MM`)**.

```
Модель памяти - это набор правил, являющийся частью спецификации языка программирования и/или аппаратной архитектуры, который определяет, какие значения может увидеть операция чтения из памяти.
```

***=> Модель памяти определяет и описывает допустимые поведения многопоточной программы***

И тут важно понимать, что это не просто набор рекомендаций, а строгий протокол взаимодействия между тремя сторонами : программистом, компилятором и разработчиком аппаратного обеспечения. Мы, как программисты, обязуемся использовать специальные инструменты, например такие как атомарные переменные, помечая компилятору и процессору точки, в которых стоит действовать согласно протоколу. Взамен, компилятор и процессор гарантируют, что в этих точках они не будут нарушать наши ожидания, но оставляют за собой право на любые оптимизации в остальном коде.

### The actors and the distributed system view

Чтобы понять в полной мере природу проблемы, полезно перестать думать о процессоре как о монолитной сущности. По своей сути, современный многоядерный процессор - это распределённая система.

В этой распределённой системе у нас есть :

- **независимые узлы в виде процессорных ядер**, каждое из которых имеет своё состояние (регистры) и локальные оптимизации (`store buffers`).

- **локальные кэши**

- **общая сеть** в виде системной шины и иерархии памяти, через которую узлы обмениваются информацией. Эта сеть имеет свои задержки и особенности.

И когда мы смотрим на проблему под этим неочевидным углом, становится очевидно, что MM - это по сути протокол консистентности для этой распределённой системы. То есть модели памяти - это не про `reordering`'и и `memory_order`'ы, которые часто все пытаются заучить как стих, в реальности всё куда сложнее. И для полноценного понимания механизма, нужно думать не в категориях переупорядочиваний, а в категорях частичных порядков отношений и гарантий, не забывая учитывать часть договора с аппаратной стороны.

***Исполнение любой многопоточной программы - это не единая временная шкала, а сложный граф событий, связанных между собой отношениями причинности***. Различные `memory_order::` - это лишь инструменты, используя которые, мы добавляем в этот граф недостающие рёбра (отношения записей и чтений), чтобы сделать исполнение программы предсказуемым и эффективным.


## A Classification : Hardware and Declarative MMs

Поскольку модель памяти является протоколом между программистом, компилятором и аппаратной архитектурой, её гарантии должны поддерживаться на всех уровнях этой иерархии. В связи с этим, модели памяти классифицируются на два ключевых класса : Декларативные модели памяти и Аппаратные.

- Аппаратная модель памяти - это спецификация, определяющая гарантии порядка операций, которые предоставляет разработчик конкретной архитектуры процессора. Аппаратная модель определяет самое слабое поведение, которое может продемонстрировать процессор в смысле переупорядочиваний инструкции и видимости записей. Эта модель в большей степени связывает именно аппаратуру и компилятор, так как без неё весь математический формализм Декларативных моделей памяти был бы бесполезен с практической точки зрения.

- Декларативная модель памяти - это фактически высокоуровневая абстракция, формальная математическая модель. Она не описывает фактическое исполнение на конечной машине, а описывает правила и гарантии, на которые опирается программист. Подобные модели (например модель памяти C++) оперируют *частичными порядками, отношениями причинности*, о которых мы поговорим позже. Однако важно понимать, что **сложный математический формализм декларативной модели является следствием сложного устройства аппаратной реализации современных чипов процессоров**. То есть проблема её сложности скорее в истории развития и природе аппаратуры, а не в желании математиков всё формализовать абстракциями :( 

В итоге, мы как программисты видим иерархию соглашений : декларативная модель реализуется поверх аппаратной модели. Задача компилятора - транслировать абстрактные гарантии, которые требует программист в конкретных точках программы (например аннотируя типы как атомарные), в конкретные аппаратные инструкции (например барьеры памяти), которые необходимы для обеспечения этих гарантий на данном железе.

---

В моём изложении я буду придерживаться этого разделения, стараясь указывать о каком классе MM мы ведём речь. Для начала, мы опишем аппаратную модель, чтобы понять физические причины, которые лежат в основе высших формализмов. Затем, мы поднимемся на уровень языков, чтобы научиться рассуждать и доказывать поведение и корректноость программ, не держа в голове детали реализации на конкретной архитектуре процессоров.

---
## The physical reality : Hardware MMs.

### The Memory Wall

В основе всех сложностей современных MM лежит один физический факт : колоссальная разница в производительности между процессорными ядрами и основной памятью. Эта проблема известна как `Memory Wall`. Современное процессорное ядро способно выполнить сотни инструкций за то время, пока данные идут из DRAM и обратно. Конечно, уже как десятки лет у каждого ядра есть иерархия кэшей, которая фактически решила эту проблему в определённых случаях. Однако архитектурной сложности, возникли и серьёзные накладные расходы на синхронизацию и протоколы когерентности для поддержания распределенной системы из кэшей в детерминированном согласованном состоянии. Они обеспечивают, что запись, сделанная одним ядром, в конце концов сделает невалидными (или обновит) копии этой же кеш-линии в кешах других ядер.

Подробно про протоколы когерентности было [рассказано тут](https://github.com/Niko256/MyNotes/blob/main/Concurrency/Atomic%20operations.md#cache-coherency).

Однако для нашей текущей перспективы важно не само устройство кэшей, протоколов когерентности, а гарантия, которую они предоставляют : протоколы когерентности устанавливают глобальный, единый для всех ядер порядок операций для *каждой отдельной кэш-линии*. То есть фактически все ядра гарантированно договариваются о том, в каком порядке происходили записи в конкретную ячейку памяти `X`. 

И эта гарантия, называемая *когерентностью*, нам очень важна. Ведь например, она *запрещает* сценарий, при котором поток `T#3` читает сначала запись `X = 1`, а затем `X = 3`, в то время как поток `T#4` видит эти записи в обратном порядке. !НО! : гарантия когерентности *ничего* не говорит о порядке чтения в разные кэш-линии, например `X` и `Y`.

И конечно же, протокол когерентности - очень дорогостаящая операция по задержке системной шины, требующая обмена сообщениями между ядрами. И как раз стремления скрыть эти задержки, связанные с этими протоколами, являются первопричиной большинства неопределённых поведений, с которыми мы сталкиваемся.

### The store buffer

Чтобы избежать серьёзных простоев ядер процессоров в ожидании разрешения конфиктов в процессе исполнения протоколов когерентности, современные архитектуры процессоров имеют внутри себя аппаратную оптимизацию в виде буфера записи (`store buffer`).

Концептуально, `store buffer` представляет из себя небольшую приватную `FIFO`-очередь для каждого процессорного ядра. 

Когда ядро выполняет инструкцию записи (store), эта инструкция не блокирует его. Вместо этого процессор помещает данные о записи (целевой адрес и записываемое значение) в свой буфер и немедленно переходит к выполнению следующей инструкции в потоке. Это позволяет конвейеру процессора продолжать работу на полной скорости, не обращая внимания на задержки, связанные с медленным процессом согласования кэшей.

В это же время, в асинхронном режиме, специальный аппаратный блок внутри ядра (`LSU/MOB`), специализрованный на логике управления памятью, который отправляет запись контроллеру кэша. Эта логика делегирует задачи когерентности контроллеру `L1` кеша, но сама управляет очередностью и моментом фиксации записи в кеше. Это позволяет вычислительному конвейеру ядра работать без остановок. Он начинает обрабатывать записи из этого буфера. Для каждой записи, находящейся в очереди, контроллер кэша внутри ядра выполняет все необходимые шаги протокола когерентности - например, отправляет сообщения об инвалидации кэш-линии или запрос на эксклюзивное владение другим ядрам. Только после получения всех необходимых подтверждений и получения кеш-линии в нужном состоянии, запись из буфера переносится в `L1` кэш ядра. И лишь с этого момента данная запись становится видимой для других процессорных ядер. 

То есть в итоге, каждая операция записи для конкретного ядра разбивается на две фазы.

![](https://i.imgur.com/XhkxpMq.png)
#### Store forwarding

Размещение записей в `store buffer` немедленно создаёт вопрос : "а что, если ядро попытается прочитать значение, которое оно только что записало, и которое всё ещё находится в буфере записи, не дойдя до кэша?"

Например :

``` C++
a = 1;
b = a + 1;
assert (b == 2);
```

Предположим, что кэш-линия, содержащая `a`, находится в разделённом владении с другим ядром. При выполнении `a = 1`, процессор отправляет эту запись в `store buffer`. Если бы при следующей инструкции `b = a + 1`, ядро бы обратилось напрямую в кэш, оно бы прочитало старое значение `a`, и `assert(b == 2)` бы провалился.

![](https://i.imgur.com/zbnlJ8C.png)


Чтобы этого избежать, процессоры реализуют механизм, известный как `store forwaring`. Когда ядро выполняет операцию чтения, оно сначала проверяет свой `store buffer` в поиске записей по тому же адресу. Если такая запись найдена, ядро использует самое свежее значение из буфера. Это гарантирует, что каждое ядро всегда видит свои собственные записи в том порядке, в котором они были выполнены, сохраняя видимость последовательного выполнения в рамках одного ядра.

#### Asymmetric visibility

Именно комбинация `store buffering` + `store forwarding` создаёт важную асимметрию, которая приводит к необходимости строгой аппаратной модели памяти.


**С точки зрения ядра, его собственные записи видны ему незамедлительно.**

**С точки зрения всех остальных ядер, эти же записи становятся видимыми с неопределённой задержкой.**

Эта асимметрия видимости является основным компромиссом между производительностью и простотой модели исполнения. Она позволяет каждому ядру работать быстро, но нарушает интуитивную понятную для всех модель Последовательной согласованности (`Sequential Consistency`). Именно для управления этой сложностью и были созданы барьеры памяти - явные инструкции, которые позволяют сообщить процессору, когда именно необходимо синхронизировать своё локальное состояние с глобальным состоянием системы (например принудительно протолкнув содержимое буфера записи в кэш и дождавшись завершения этой операции).

---
