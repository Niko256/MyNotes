
## The End of the Single-threaded fairy tale.

### The `as-if` rule.

В мире однопоточной среды исполнения, разработчики десятилетиями жили под защитой простого соглашения, известного как **принцип `as-if`**.

[Этот принцип достаточно краток и понятен](https://en.wikipedia.org/wiki/As-if_rule) : компилятор языка программирования и процессор имеют право производить любые преобразования и оптимизации программы : менять инструкции местами, удалять избыточные инструкции, кэшировать значения в регистрах - до тех пор, пока наблюдаемое поведение программы в рамках одного потока остаётся таким же, как если бы она исполнялась в точности так, как было написано в её исходном коде.

Для нас это был идеальный мир, в котором можно было писать простой, последовательный код, а компилятор и новое поколение процессоров брали на себя магию его оптимизации и ускорения. Детали этих преобразований оставались скрытыми от нас. Главное, что конечный результат был корректным и предсказуемым.

### The multi-core revolution

Эта модель, в которой оптимизации были невидимым благом, закончилась с наступлением эры многоядерных процессоров. Появление множества ядер, а с ними и потоков исполнения, добавило в систему *сторонних наблюдателей*. И эти наблюдатели смогли увидеть то, что раньше было скрыто внутри принципа `as-if`, а именно внутренние трюки компилятора и процессора. Оптимизации, абсолютно безопасные для однопоточной среды, внезапно стали приводить к видимым и зачастую катастрофическим последствиям.

Рассмотрим классический пример, демонстрирующий суть проблемы. Предположим, что мы в одном потоке подготавливаем какой-то ресурс, а затем выставляем флаг, сигнализируя второму потоку, что ресурс готов.

``` C++
int x = 0;
bool done = false;

/* Thread#1 */
x = 42;

/* Thread#2 */
while (!done) {
	done = true;
}

std::cout << x << std::endl; // expecting 42 here...
```

Наше намерение здесь чисто: второй поток должен увидеть `x = 42`. В однопоточной среде порядок `x = 42` и `done = true` был бы незыблем. Но с точки зрения компилятора, оптимизирующего первый поток, переменные `x` и `done` никак не связаны между собой. Он может счесть для себя более эффективным, что сначала эффективней будет выполнить `done = true`, а затем уже `x = 42` (например для более оптимального использования регистров). Такое переупорядочивание само по себе не нарушает логику первого потока, но для второго потока оно фатально : он *может* прочитать `done = true`, выйти из цикла и прочитать из `x` старое значение `0`.

И даже если компилятор сгенерирует инструкции в правильном порядке, то нам также не стоит забывать, что многоядерные процессоры - это невероятно сложные системы с аппаратной точки зрения и они также являются авторами проблем в конкурентной среде. Далее мы разберём проблему детальнее, но вообще говоря, процессорное ядро, выполнив запись `x = 42`, скорее всего, отложит эту запись в специальный *буфер*, чтобы не простаивать при следующем обращении. А вот следующая запись `done = true` может быть выполнена и стать видимой для остальных ядер быстрее, чем первая запись. Снова получаем тот же результат : второй поток видит флаг, но не видит *вроде-как* записанного ресурса.

### So, what is a MM?

Именно этот конфликт между нашими желаниями предсказуемого порядка выполнения и стремлением системы к производительности, привёл человечество к необходимости формализации нового контракта. Этот контракт и есть **Модель памяти (`MM`)**.

```
Модель памяти - это набор правил, являющийся частью спецификации языка программирования и/или аппаратной архитектуры, который определяет, какие значения может увидеть операция чтения из памяти.
```

***=> Модель памяти определяет и описывает допустимые поведения многопоточной программы***

И тут важно понимать, что это не просто набор рекомендаций, а строгий протокол взаимодействия между тремя сторонами : программистом, компилятором и разработчиком аппаратного обеспечения. Мы, как программисты, обязуемся использовать специальные инструменты, например такие как атомарные переменные, помечая компилятору и процессору точки, в которых стоит действовать согласно протоколу. Взамен, компилятор и процессор гарантируют, что в этих точках они не будут нарушать наши ожидания, но оставляют за собой право на любые оптимизации в остальном коде.

### The actors and the distributed system view

Чтобы понять в полной мере природу проблемы, полезно перестать думать о процессоре как о монолитной сущности. По своей сути, современный многоядерный процессор - это распределённая система.

В этой распределённой системе у нас есть :

- **независимые узлы в виде процессорных ядер**, каждое из которых имеет своё состояние (регистры) и локальные оптимизации (`store buffers`).

- **локальные кэши**

- **общая сеть** в виде системной шины и иерархии памяти, через которую узлы обмениваются информацией. Эта сеть имеет свои задержки и особенности.

И когда мы смотрим на проблему под этим неочевидным углом, становится очевидно, что MM - это по сути протокол консистентности для этой распределённой системы. То есть модели памяти - это не про `reordering`'и и `memory_order`'ы, которые часто все пытаются заучить как стих, в реальности всё куда сложнее. И для полноценного понимания механизма, нужно думать не в категориях переупорядочиваний, а в категорях частичных порядков отношений и гарантий, не забывая учитывать часть договора с аппаратной стороны.

***Исполнение любой многопоточной программы - это не единая временная шкала, а сложный граф событий, связанных между собой отношениями причинности***. Различные `memory_order::` - это лишь инструменты, используя которые, мы добавляем в этот граф недостающие рёбра (отношения записей и чтений), чтобы сделать исполнение программы предсказуемым и эффективным.


## A Classification : Hardware and Declarative MMs

Поскольку модель памяти является протоколом между программистом, компилятором и аппаратной архитектурой, её гарантии должны поддерживаться на всех уровнях этой иерархии. В связи с этим, модели памяти классифицируются на два ключевых класса : Декларативные модели памяти и Аппаратные.

- Аппаратная модель памяти - это спецификация, определяющая гарантии порядка операций, которые предоставляет разработчик конкретной архитектуры процессора. Аппаратная модель определяет самое слабое поведение, которое может продемонстрировать процессор в смысле переупорядочиваний инструкции и видимости записей. Эта модель в большей степени связывает именно аппаратуру и компилятор, так как без неё весь математический формализм Декларативных моделей памяти был бы бесполезен с практической точки зрения.

- Декларативная модель памяти - это фактически высокоуровневая абстракция, формальная математическая модель. Она не описывает фактическое исполнение на конечной машине, а описывает правила и гарантии, на которые опирается программист. Подобные модели (например модель памяти C++) оперируют *частичными порядками, отношениями причинности*, о которых мы поговорим позже. Однако важно понимать, что **сложный математический формализм декларативной модели является следствием сложного устройства аппаратной реализации современных чипов процессоров**. То есть проблема её сложности скорее в истории развития и природе аппаратуры, а не в желании математиков всё формализовать абстракциями :( 

В итоге, мы как программисты видим иерархию соглашений : декларативная модель реализуется поверх аппаратной модели. Задача компилятора - транслировать абстрактные гарантии, которые требует программист в конкретных точках программы (например аннотируя типы как атомарные), в конкретные аппаратные инструкции (например барьеры памяти), которые необходимы для обеспечения этих гарантий на данном железе.

---

В моём изложении я буду придерживаться этого разделения, стараясь указывать о каком классе MM мы ведём речь. Для начала, мы опишем аппаратную модель, чтобы понять физические причины, которые лежат в основе высших формализмов. Затем, мы поднимемся на уровень языков, чтобы научиться рассуждать и доказывать поведение и корректноость программ, не держа в голове детали реализации на конкретной архитектуре процессоров.

---
## The physical reality : Hardware MMs.

### The Memory Wall

В основе всех сложностей современных MM лежит один физический факт : колоссальная разница в производительности между процессорными ядрами и основной памятью. Эта проблема известна как `Memory Wall`. Современное процессорное ядро способно выполнить сотни инструкций за то время, пока данные идут из DRAM и обратно. Конечно, уже как десятки лет у каждого ядра есть иерархия кэшей, которая фактически решила эту проблему в определённых случаях. Однако архитектурной сложности, возникли и серьёзные накладные расходы на синхронизацию и протоколы когерентности для поддержания распределенной системы из кэшей в детерминированном согласованном состоянии. Они обеспечивают, что запись, сделанная одним ядром, в конце концов сделает невалидными (или обновит) копии этой же кеш-линии в кешах других ядер.

Подробно про протоколы когерентности было [рассказано тут](https://github.com/Niko256/MyNotes/blob/main/Concurrency/Atomic%20operations.md#cache-coherency).

Однако для нашей текущей перспективы важно не само устройство кэшей, протоколов когерентности, а гарантия, которую они предоставляют : протоколы когерентности устанавливают глобальный, единый для всех ядер порядок операций для *каждой отдельной кэш-линии*. То есть фактически все ядра гарантированно договариваются о том, в каком порядке происходили записи в конкретную ячейку памяти `X`. 

И эта гарантия, называемая *когерентностью*, нам очень важна. Ведь например, она *запрещает* сценарий, при котором поток `T#3` читает сначала запись `X = 1`, а затем `X = 3`, в то время как поток `T#4` видит эти записи в обратном порядке. !НО! : гарантия когерентности *ничего* не говорит о порядке чтения в разные кэш-линии, например `X` и `Y`.

И конечно же, протокол когерентности - очень дорогостаящая операция по задержке системной шины, требующая обмена сообщениями между ядрами. И как раз стремления скрыть эти задержки, связанные с этими протоколами, являются первопричиной большинства неопределённых поведений, с которыми мы сталкиваемся.

### The store buffer

Чтобы избежать серьёзных простоев ядер процессоров в ожидании разрешения конфиктов в процессе исполнения протоколов когерентности, современные архитектуры процессоров имеют внутри себя аппаратную оптимизацию в виде буфера записи (`store buffer`).

Концептуально, `store buffer` представляет из себя небольшую приватную `FIFO`-очередь для каждого процессорного ядра. 

Когда ядро выполняет инструкцию записи (store), эта инструкция не блокирует его. Вместо этого процессор помещает данные о записи (целевой адрес и записываемое значение) в свой буфер и немедленно переходит к выполнению следующей инструкции в потоке. Это позволяет конвейеру процессора продолжать работу на полной скорости, не обращая внимания на задержки, связанные с медленным процессом согласования кэшей.

В это же время, в асинхронном режиме, специальный аппаратный блок внутри ядра (`LSU/MOB`), специализрованный на логике управления памятью, который отправляет запись контроллеру кэша. Эта логика делегирует задачи когерентности контроллеру `L1` кеша, но сама управляет очередностью и моментом фиксации записи в кеше. Это позволяет вычислительному конвейеру ядра работать без остановок. Он начинает обрабатывать записи из этого буфера. Для каждой записи, находящейся в очереди, контроллер кэша внутри ядра выполняет все необходимые шаги протокола когерентности - например, отправляет сообщения об инвалидации кэш-линии или запрос на эксклюзивное владение другим ядрам. Только после получения всех необходимых подтверждений и получения кеш-линии в нужном состоянии, запись из буфера переносится в `L1` кэш ядра. И лишь с этого момента данная запись становится видимой для других процессорных ядер. 

То есть в итоге, каждая операция записи для конкретного ядра разбивается на две фазы.

![400](https://i.imgur.com/XhkxpMq.png)
#### Store forwarding

Размещение записей в `store buffer` немедленно создаёт вопрос : "а что, если ядро попытается прочитать значение, которое оно только что записало, и которое всё ещё находится в буфере записи, не дойдя до кэша?"

Например :

``` C++
a = 1;
b = a + 1;
assert (b == 2);
```

Предположим, что кэш-линия, содержащая `a`, находится в разделённом владении с другим ядром. При выполнении `a = 1`, процессор отправляет эту запись в `store buffer`. Если бы при следующей инструкции `b = a + 1`, ядро бы обратилось напрямую в кэш, оно бы прочитало старое значение `a`, и `assert(b == 2)` бы провалился.

![400](https://i.imgur.com/zbnlJ8C.png)


Чтобы этого избежать, процессоры реализуют механизм, известный как `store forwaring`. Когда ядро выполняет операцию чтения, оно сначала проверяет свой `store buffer` в поиске записей по тому же адресу. Если такая запись найдена, ядро использует самое свежее значение из буфера. Это гарантирует, что каждое ядро всегда видит свои собственные записи в том порядке, в котором они были выполнены, сохраняя видимость последовательного выполнения в рамках одного ядра.

#### Asymmetric visibility

Именно комбинация `store buffering` + `store forwarding` создаёт важную асимметрию, которая приводит к необходимости строгой аппаратной модели памяти.


**С точки зрения ядра, его собственные записи видны ему незамедлительно.**

**С точки зрения всех остальных ядер, эти же записи становятся видимыми с неопределённой задержкой.**

Эта асимметрия видимости является основным компромиссом между производительностью и простотой модели исполнения. Она позволяет каждому ядру работать быстро, но нарушает интуитивную понятную для всех модель Последовательной согласованности (`Sequential Consistency`). Именно для управления этой сложностью и были созданы барьеры памяти - явные инструкции, которые позволяют сообщить процессору, когда именно необходимо синхронизировать своё локальное состояние с глобальным состоянием системы (например принудительно протолкнув содержимое буфера записи в кэш и дождавшись завершения этой операции).

#### Invalidate Queues and other Optimizations

Хотя `store buffers` являются основным и наиболее понятным источником переупорядочивания видимостей, они не единственны. В стремлении к производительности, процессорные архитектуры применяют и другие оптимизации, которые также могут привести к неопределённому поведению.

Одной из таких оптимизаций является *очередь инвалидации (`invalidate queue`)*. Вспомним, что для того, чтобы ядро `A` могло записать в кэш-линию, оно должно сначала инвалидировать копии линии в кэшах других ядер. Этот процесс требует от ядра `B` получения сообщения об инвалидации, применения его к соответствующему кэшу и отправки подтверждения. Обработка сообщения об инвалидации может занять некоторое время, особенно если кэш в этот момент занят другими операциями.

Чтобы не заставлять ядро `A` ждать, пока ядро `B` реально выполнит инвалидацию, иногие архитектуры позволяют ядру `B` сделать опредённый трюк. Получив сообщение, ядро может немедленно поместить его в специальную небольшую очередь `invalidate queue` - и тут же отправить подтверждение, мол "инвалидация окончена". Ядро `A`, получив быстрое подтверждение, прожолжает свою работу, а ядро `B` обязуется обработать сообщения из своей очереди инвалидации в ближайшем будущем.

![400](https://i.imgur.com/UoUCyOu.png)
То есть мы получаем ситуацию, в которой ядро `B` может продолжать читать устаревшее значение из своего кэша даже после того, как оно уже подтвердило ивалидацию этой кэш-линии. Эта отложенная инвалидация создаёт ещё одну видимость, в которой порядок операций, наблюдаемый разными ядрами, расходится.

---
### Memory Barriers (Fences)

Итак, мы видим, что архитектура процессора по умолчанию предоставляет нам слабую модель памяти (`relaxed mm`), в которой порядок видимости операций не гарантирован. Чтобы этот строгий порядок восстановить в определённых точках, где это критически необходимо, процессоры предоставляют специальный набор инструкций, а именно барьеры памяти.

Аппаратный барьер памяти - это явная инструкция, которая принудительно на аппаратном уровне устанавливает отношение порядка между операциями после него. Подробнее про барьеры можно почитать у меня [тут](https://github.com/Niko256/MyNotes/blob/main/Concurrency/Barriers.md).

---
## The Abstraction : Declarative MMs

Мы установили, что разные аппаратные архитектуры предоставляют определённые гарантии на уровне инструкций. Однако писать переносимый, человекочитаемый и корректный параллельный код, полагаясь исключительно на низкоуровневые барьеры памяти и знание конкретной архитектуры - это слишком непосильная и насыщенная ошибками задача.

Эта модель служит двум целям. Во-первых, она даёт нам абстрактный, платформо-независимый набор правил для рассуждения о поведении программы. Во-вторых, она служит спецификацией для разработчиков компиляторов, указывая им как транслировать высокоуровневые инструкции языка, связанные с обращением к памяти, в машинные инструкции.

### Sequential Consistency

Для построения работующего контракта, необходимо иметь точку отсчёта - некоторый идеал, к которому мы будем устремлять поведение наших программ. В мире моделей памяти таким идеалом **Последовательная согласованность (`Sequential Consistency, SC`)**.

Формальное определение `SC`, данное Лэсли Лэмпордом в 1979 году, звучит следующим образом :

```

...результат любого исполнения тот же самый, как если бы операции всех процессоров (потоков) выполнялись в некотором последовательном порядке, а операции каждого отдельного процессора (потока) появлялись бы в этой последовательности в порядке, определённом этой программой. (Lamport, 1979, How to make a Multiprocesor Computer that correctly executes Multiprocess programs).
```

Проще говоря, модель `SC` представляет исполнение программы как простое перемешивание инструкций (`interleaving`) из разных потоков, при котором сохраняется их исходный порядок внутри каждого потока. Эта модель очень интуитивна и является золотым стандартом предсказуемости поведения.

#### The High Cost of Pure Sequential Consistency

Если `SC` так проста, то почему бы просто не потребовать от всех языков и процессоров её реализации? Ответ конечно же кроется в производительности.

Требование строгой `SC` для каждой операции с памятью по сути запрещает почти все аппаратные и компиляторные оптимизации, которые мы обсуждали ранее :

- `SC` запрещает ядру использовать `store buffer`, т.к. любая запись должна становиться глобально видимой до выполнения следующей инструкции.

- `SC` запрещает очередь инвалидации, т.к. инвалидация должна происходить немедленно.

- `SC` запрещает компилятору переупорядочивать независимые обращения к памяти (например `X = 1`, `r1 = Y`), т.к. это нарушает программный порядок в глобальном `interleaving`.

Строгая реализация `SC` также означала бы, что после каждого обращения к памяти, процессор должен был бы устанавливать полный барьер, что свело бы на нет все преимущества многоядерности. Системы, предоставляющие такую гарантию, были бы неконкурентоспособно медленными.

Думаю уже становится понятно, к чему я веду. Видно, что в этой ситуации необходим компромисс, который позволил бы сохранить простоту и предсказуемость `SC` для корректных программ, но при этом дал бы свободу компилятору и аппаратуре применять оптимизации так, где это безопасно. На этом компромиссе и основаны современные декларативные модели памяти.

### `DRF-SC` (Data-Race-Free implies SC Guarantee)

Компромисс, который был найден и лёг в основу моделей памяти `Java, C++, Swift`, известнен под названием `DRF-SC`. Это центральная идея всех современных декларативных моделей памяти. Фактически, является главным пунктном протокола между программистом и системой (компилятор + процессор).

Формулируется контракт следующим образом :

```
Если программа написана так, что в ней отсутствуют гонки данных, то система гарантирует, что любое её исполнение будет эквивалентно некоторому последовательно согласованному исполнению (Sequential Consistency). Если же в программе есть гонка, её поведение не определено (UB). (Boegm & Adve, "Foundations of the C++ Concurrency Memory Model").
```

Важнейшее следствие этого контракта - если в нашей программе нет гонок, нам не нужно думать о аппаратных оптимизациях или специфике архитектур. Мы можем рассуждать о своей программе так, словно все инструкции просто перемешиваются в едином временном потоке. Система берёт на себя всю грязную работу по расстановке необходимых барьеров и запрету небезопасных оптимизаций, чтобы поддерживать иллюзию строгого `SC` для нас.

Однако, если программа содержит гонку, этот контракт аннулируется. Система снимает с себя все обязательства. 

Этот подход позволяет системе применять агрессивные оптимизации по умолчанию, исходя из предположения, что код написан без гонок. Обязанность доказать системе обратное, расставив атомарные маркеры, ложится на нас, программистов.

#### Defining a Data Race

Модель `DRF-SC` будет не более чем размахиваниями руками, если строго не сформулировать понятие `Data Race`. Это определение состоит из двух частей.

Сначала введём понятие конфликта : 

```
Два обращения к памяти конфликтуют, если они обращаются к одной и той же ячейке памяти, и по крайней мере одно из этих обращений - запись.
```

( => два чтения из одной ячейки не конфликтуют, а вот чтение и запись - конфликтуют. Две записи также конфликтуют).

Имея это определение, мы наконец можем сформулировать что такое `Data Race` :

```
Гонка Данных (Data Race) - это два конфликтующих неатомарных обращения к памяти из разных потоков, которые не упорядочены между собой отношением H-B.
```

Здесь упоминается ` не упорядочены через H-B`. В формальной модели `C++` это означает, что между двумя обращениями нет отношения `happens-before`, которое мы формально введём чуть позже.

=> Задачей программиста, стремящегося к `DRF-SC` является расстановка достаточного количества синхронизирующих операций для установления отношения `happens-before` между всеми потенциально конфликтующими обращениями.


### The C++ MM : A Formal View



// ...

``` C++
// SB+sc_sc+sc_sc
// Store Buffering (or Dekker's), with all four accesses SC atomics
// Question: can the two reads both see 0 in the same execution?
int main() {
  atomic_int x=0; atomic_int y=0;
  {{{ { y.store(1,memory_order_seq_cst);
        r1=x.load(memory_order_seq_cst); }
  ||| { x.store(1,memory_order_seq_cst);
        r2=y.load(memory_order_seq_cst); }  }}}
  return 0;
}
```

![300](https://i.imgur.com/2PjT0Pc.png)
