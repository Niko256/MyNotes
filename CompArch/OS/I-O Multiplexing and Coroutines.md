## Why?

Как упоминалось в заметках о файберах, при построении сетевых приложений, основной задачей является эффективная обработка большого количества активных клиентских соединений.

В модели `thread-per-connection`, для каждого принятого соединения (`accept`) создаётся отдельный системный поток, который в синхронном, блокирующем режиме обслуживает весь жизненный цикл этого соединения : чтение запроса (`read`), обработку данных и отправку ответа (`write`). В этом подходе логика обработки одного клиента инкапсулируется в одной функции, а состояние этого клиента хранится в локальных переменных потока, на его стеке.

И конечно, как нам известно, эта модель не масштабируется (в силу дороговизны переключения контекста потоков, объёмов стеков потоков, и.т.д.).

Эта проблема под названием `C10k problem`, поставила перед индустрией задачу обслуживания десятков тысяч одновременных соединений, используя ограниченное число системных потоков (или вообще используя единственный поток).

Решение этой задачи требует большого пересмотра модели взаимодействия с операциями ввода-вывода. Необходимо было найти способ, при котором один поток мог бы эффективно управлять множеством соединений, не блокируясь в ожидании завершения операции.

## Blocking and Non-blocking syscalls

Для понимания асинхронной модели необходимо сначала рассмотреть поведение стандартных системных вызовов для работы с операциями ввода-вывода. В `Unix` сетевые сокеты представлены в виде файловых дескрипторов. Операции над ними, такие как `read()`, `write()`, `accept()`, по умолчанию работают в блокирующем режиме. Это означает, что когда приложение вызывает, например, `read()` для чтения данных из сокета, а в буфере этого сокета данных ещё нет, ядро операционной системы переводит вызывающий поток в состояние ожидания (`sleep`). Поток удаляется из очереди готовых к исполнению, и планировщик не будет выделять ему процессорное время до тех пор, пока не наступит ожидаемое событие - а именно поступление данных в сокет. Для потока, обслуживающего единственное соединение, такое поведение является логичным и эффективным. Однако для потока, который должен обрабатывать тысячи соединений, блокировка на одном из них является фатальной и делает невозможной обработку всех остальных событий.

Основной сдвиг парадигмы заключается в переключении файловых дескрипторов в неблокирующий режим. Это достигается установкой флага `O_NONBLOCK` для дескриптора [с помощью системного вызова](https://www.man7.org/linux/man-pages/man2/fcntl.2.html) `fcntl()`. После такой модификации поведение системных вызовов ввода-вывода ортогонально меняется. Если теперь приложение вызовет `read()` на неблокирующем сокете, а данных в нем нет, то системный вызов не заблокирует поток. Вместо этого он немедленно вернет управление приложению, а в качестве результата возвратит `-1` с установкой глобальной переменной `errno` в одно из значений `EAGAIN/EWOULDBLOCK`. Это служит сигналом от операционной системы, что данная операция не может быть выполнена в данный момент и стоит попробовать позже. Аналогичное поведение демонстрируют и другие вызовы, например, `accept()` вернет `EAGAIN`, если в очереди нет новых входящих соединений.

Такой механизм позволяет одному потоку пытаться выполнить операции ввода-вывода на множестве сокетов, не рискуя быть заблокированным.

Однако это порождает новую проблему : как определить тот самый момент, когда наступило это самое "позже"? Наивное решение в виде бесконечного цикла, в котором приложение перебирает все дескрипторы и пытается из них читать (`busy-waiting`), является крайне неэффективным, поскольку приводит к полной утилизации процессорного ядра без выполнения полезной работы. 

Необходим механизм, который бы позволял приложению "заснуть" и получить уведомление от ядра именно тогда, когда один или несколько из интересующих его дескрипторов становятся готовы к выполнению операции. УРА : мы изобрели **мультиплексирование ввода-вывода (`I/O Multiplexing`)**.

---
## `Epoll : Reactor on the Kernel layer`.

Для необходимого решения задачи ожидания на множестве дескрипторов в `Linux` был разработан специализированный `API` - `epoll`. `epoll` - это семейство системных вызовов мультиплексирования, пришедший на смену менее масштабируемым предшественникам, таким как `select()` и `poll()`. Ключевым недостатком предшественников была их сложность $O(N)$, где $N$ - общее число отслеживаемых дескрипторов. При каждом вызове приложению приходилось передавать в ядро весь список интересующих его дескрипторов, а ядро было вынуждено линейно проверять каждый из них. После возврата из вызова приложение также должно было само итерироваться по всему списку, чтобы найти те несколько дескрипторов, которые стали готовы.

Однако, `epoll` использует принципиально иной, управляемый событиями подход, который обеспечивает сложность $O(1)$ по отношению к общему числу отслеживаемыз дескрипторов.

Работа с `epoll` строится вокруг трёх системных вызовов : 

- `epoll_create1()` : этот системный вызов создаёт в пространстве ядра специальный объект - экземпляр `epoll` и возвращает файловый дескриптор, который его идентифицирует. Этот объект будет хранить в себе информацию обо всех отслеживаемых дескрипторах. Важно, что этот объект персистентен и создаётся один раз на весь жизненный цикл приложения или потока-обработчика.

- `epoll_ctl()` : этот системный вызов модифицирует список, хранящийся в экземпляре `epoll`. С помощью этого вызова приложение регистрирует (`EPOLL_CTL_ADD`), изменяет (`EPOLL_CTL_MOD`) или удаляет (`EPOLL_CTL_DEL`) файловые дескрипторы из списка отслеживаемых.

- `epoll_wait()` : этот системный вызов является блокирующим (но с возможностью таймаута). Он является сердцем цикла событий. Поток, вызвавший `epoll_wait()`, блокируется до тех пор, пока **на любом** из зарегистрированных в экземпляре `epoll` дескрипторов не произойдёт интересующее событие. Когда событие наступает, ядро пробуждает поток, и `epoll_wait()` возвращает только список тех дескрипторов, которые стали готовы к выполнению операций с ними.


Масштабируемость `epoll` достигается за счёт того, что ядро само ведёт внутренний список готовых дескрипторов для каждого экземпляра `epoll`. Когда на каком-либо устройстве (например на сетевой карте) происходит событие ввода-вывода, соответствующий драйвер проверяет, зарегистрирован ли данный файловый дескриптор в каком-либо экземпляре `epoll`. Если да, то ссылка на этот дескриптор добавляется во внутренний список готовых. Таким образом, вызов `epoll_wait()` сводится лишь к проверке наличия элементов в этом списке (очереди) и их возврату приложению. Его производительность зависит не от общего числа отслеживаемых дескрипторов, а только от количества реально произошедших событий.

Как уже стало очевидно, именно семейство `epoll` (и ему подобных) системных вызовов, лежит в основе всех современных асинхронных фреймворков.

---
## Reactor and Event Loop patterns


Немного вернёмся к блокирующей синхронной модели, чтобы плавно прийти к устройству и реализации асинхронной неблокирующей модели, через механизмы, которые мы уже описали.

Классический блокирующий эхо-сервер представляет собой простейший, но показательный пример последовательного потока управления. Его логика инкапсулирована в бесконечном цикле, который обслуживает клиентов строго одного за другим.

``` C
// Концептуальный код синхронного блокирующего сервера
int server_fd = socket(AF_INET, SOCK_STREAM, 0);
// ... bind() ... listen() ...

while (true) {
    // 1. Первая точка блокировки : ожидание нового соединения
    int client_fd = accept(server_fd, ...);

    // 2. Вторая точка блокировки: ожидание данных от клиента
    char buffer[1024];
    ssize_t bytes_read = read(client_fd, buffer, 1024);

    if (bytes_read > 0) {
        // Логика обработки
        // ...
        // 3. Третья точка блокировки : ожидание, пока операционная система примет данные в буфер на отправку
        write(client_fd, buffer, bytes_read);
    }
    
    close(client_fd);
}
```

Для преодоления ограничений блокирующей модели применяется архитектурный паттерн **`Reactor`**. Он формализует архитектуру приложения, которое реагирует на события, поступающие извне. Его основная задача - принимать события от одного или нескольких источников, демультиплексировать их и отправлять соответствующим обработчикам для выполнения. Этот паттерн позволяет отделить логику, специфичную для конкретной задачи (обработчики), от общего механизма приёма и диспатчеризации событий.

Классическая структура паттерна `Reactor` включает в себя несколько основных компонент : 

- Дескрипторы
- Демультиплексор событий
- Обработчики событий
- Реактор

А теперь конкретизируем эти компоненты в нашем контексте. 

Дескрипторами в нашей задаче являются файловые дескрипторы, представляющие резурсы операционной системы. *Это источники событий*.

Роль демальтиплексора событий в нашей архитектуре выполняет системный вызов `epoll_wait()`. Он ожидает наступления событий на множестве дескрипторов.

Обработчиками событий (`callbacks/handlers`) являются компоненты приложения, реализующие конкретную логику при наступлении определённого события. В коде приложения это может быть функция, метод объекта или замыкание.

Реактор является центральным компонентом, который управляет всем процессом. Он предоставляет интерфейс для регистрации и удаления обработчиков событий. Основная его функция - запуск `event-loop` - бесконечного цикла, внутри которого он вызывает демультиплексор событий для ожидания, а после получения готовых дескрипторов - вызывает (диспатчеризует) связанные с ними обработчики.


Важно сделать критическое замечание : сам по себе вызов `epoll_wait()` является синхронным и ***блокирующим*** по отношению к потоку, который его исполняет. Это может показаться парадоксальным, но именно эта блокировка делает модель цикла событий эффективной. Поток не тратит ресурсы процессора впустую, а засыпает, передавая управление ядру операционной системы. Он будет пробуждён только тогда, когда появится реальная работа (наступят события). Таким образом, `epoll_wait()` блокирует один управляющий поток для того, чтобы реализовать асинхронную, неблокирующую обработку событий для приложения в целом. 

---
## Decomposition 

Теперь можно разобрать ключевую идею трансформации одной модели в другую.

Мы берём линейный код синхронного сервера и разрезаем его в точках, где происходили блокирующие вызовы операций ввода-вывода. Каждый полученный фрагмент оборачивается в обработчик (`callback`). Вместо того, чтобы вызывать следующую операцию немедленно, мы инициируем асинхронную операцию и предоставляем асинхронному фреймворку наш callback, который тот должен будет вызвать, когда событие произойдёт.

``` C++
// Decomposition
class AsyncServer {
    void start_accept() {
        // Инициация асинхронного принятия соединения
        acceptor_.async_accept([this](error_code ec, tcp::socket socket) {
            if (!ec) {
                // Первый callback вызывается когда соединение принято
                on_accept(std::move(socket));
            }
            start_accept(); // Цикл продолжается (переподписка на событие)
        });
    }
    
    void on_accept(tcp::socket socket) {
        auto session = std::make_shared<Session>(std::move(socket));
        
        // Инициация асинхронного чтения
        session->start_reading([session](error_code ec, size_t bytes) {
            if (!ec) {
                // Второй callback вызывается когда данные получены
                session->on_data_received(bytes);
            }
        });
    }
};

class Session {
    void start_reading(Callback on_read) {
        // Инициация асинхронной операции чтения
        socket_.async_read_some(buffer(data_), 
            [self = shared_from_this(), on_read](error_code ec, size_t bytes) {
                // Данные доступны => вызывается callback
                on_read(ec, bytes);
            });
    }
    
    void on_data_received(size_t bytes_read) {
        // Обработка данных
        process_data(data_, bytes_read);
        
        // Инициация асинхронной записи
        start_writing(bytes_read, [this](error_code ec, size_t bytes) {
            // Третий callback вызывается когда данные отправлены
            if (!ec) {
                // Цикл продолжается - начинаем читать снова
                start_reading(...);
            }
        });
    }
    
    void start_writing(size_t length, Callback on_write) {
        boost::asio::async_write(socket_, boost::asio::buffer(data_, length),
            [self = shared_from_this(), on_write](error_code ec, size_t bytes) {
                // Данные отправлены
                on_write(ec, bytes);
            });
    }
};
```


Уже в этом коде видно проблему `callback_hell`, которая заключается в сложности организации потока управления и управления состоянием. По мере усложнения логики приложения, каждая асинхронная функция начинает проростать вложенной цепочкой обработчиков. Эта проблема носит не только синтаксический характер, но и семантический : код, который логически является последовательным, оказывается разбросанным по нескольким, не связанным напрямую обработчикам. Читать и разбирать такой код становиться контринтуитивно, поскольку порядок написания не соответствует порядку исполнения. Также, классические механизмы обработки ошибок (например исключения), оказываются бесполезными. Исключение, брошенное внутри одного обработчика, не может быть поймано в контексте, который инициировал исходную асинхронную операцию (так как стек вызовов к тому моменту уже соверщенно поменялся). Поэтому например в `Boost::Asio` вся обработка ошибок происходит через коды ошибок (в каждом обработчике), что дополнительно зашумляет код.


**То есть фактически, при использовании обработчиков программист вынужден вручную конструировать конечный автомат для логики своего приложения, где каждый обработчик представляет собой переход между состояниями**.

Стало очевидно, что необходим механизм, который бы позволил описывать последовательности асинхронных операций в привычном линейном стиле, упаковав всю сложность управления состоянием и переходами на компилятор или среду исполнения (runtime). Этим механизмом стали корутины.

## Coroutines

В общем понимании, любая подпрограмма (`subroutine`) имеет одну точку входа и одно точку выхода. Управление передаётся подпрограмме при её вызове и возвращается от неё только по её завершении (return).

Корутина (сопрограмма) является обобщением подпрограммы. Это вычислительная единица, исполнение которой может быть приостановлено (`suspend`) в определённой точке и возобновлено (`resume`) позже с той же точки, сохраняя при этом весь свой внутренний контекст и состояние.

![](https://i.imgur.com/RI9ejZP.png)

Таким образом, корутина обладает множеством точек входа и выхода. Эта способность к приостановке и возобновлению является ключевым свойством, которое делает корутины идеальным и необходимым инструментом для реализации асинхронной логики в кооперативной многозадачности.


Но всё же важно понимать, что в общем смысле - корутина это некоторый паттерн, а не конкретный тип или вид функции. Другое дело, что во многих языках программирования сущности, реализующие поведение корутины, называются также. (C++ Coroutines, Goroutines, ...). 

И далее будем рассмотривать именно сущности, реализующие поведение корутины, классифицируем их и поймём их место в нашем асинхронном мире.

Корутины можно классифицировать по двум ключевым ортогональным признакам : 

- по модели передачи управления
- по способу управления состоянием


Для начала разберёмся с моделями передачи управления. Существует два вида моделей : симметричная и ассиметрачная.

В симметричной модели все корутины равноправны. Существует единственная операция `transfer(target)`, которая приостанавливает текущую корутину и передаёт управление целевой корутине `target`. **То есть в этой модели отсутствует иерархия `caller-callee`.**


В ассиметричной модели иерархия присутствует. Существует возобновляющая сторона (`resumer`) и сама корутина. Возобновляющая сторона вызывает операцию `resume` для запуска или продолжения выполнения корутины. Корутина, в свою очередь, использует операцию `yield` (или `await`) для приостановки своего выполнения и возврата управления возобновившей её стороне. Именно эта ассиметричная  модель легла в основу современных реализаций во многих языках и фреймворках.

### Stackful vs. Stackless

Ключевое различие в практических реализациях корутин заключается в том, как именно сохраняется их состояние при приостановке. Это определяет, кто несет ответственность за построение конечного автомата - среда исполнения или компилятор.

#### Stackful

Для `Stackful` корутины выделяется полноценный системный стек. Приостановка такой корутины представляет собой полноценное переключение контекста, аналогичное тому, что делает планировщик операционной системы для потоков, но выполняемое на уровне пользовательского адресного пространства (=> значительно легковесная операция). Файтически среда исполнения при переключении сохраняет текущее состояние `callee-save` регистров, а также `stack pointer, instruction pointer` и загружает контекст другой корутины.

В случае `Stackful` корутины, конечный автомат строится средой исполнения (`runtime`). Сам код корутины компилируется как обычная функция и возможность приостановки не ограничена самой процедурой внутри корутины : она может быть приостановлена из любой вложенной функции, которую она вызывает. Весь стек вызовов будет сохранён и восстановлен при дальнейшем исполнении. Примерами являются `Goroutines (Go)` и `Boost::Coroutine`.

#### Stackless

`Stackless` корутина не требует отдельного стека для сохранения состояния. Вместо этого компилятор выполняет трансформацию сам. Функция, объявленная как корутина (например имеющая оператора `co_await/co_yield/co_return` в `C++`), анализируется компилятором, и на её основе генерируется структура (объект конечного автомата). Все локальные переменные корутины инкапсулируются в эту структуру, а тело функции разбивается на части по точкам приостановки. То есть конечный автомат строится на этапе компиляции! Эта модель чрезвычайно эффективна как по памяти (требуется лишь одна аллокация в куче для объекта состояния), так и по скорости . Единственное ограничение - корутину можно приостановить только в явно обозначенных точках

---

### Back to reality

А теперь вернёмся и посмотрим, как преображается наш асинхронный сервер при использовании `stackless` корутин, поддерживаемых в `C++20` и `Boost::Asio`. Весь клубок вложенных обработчиков распутывается и превращается в линейный, последовательный код, который выглядит почти идентично исходному блокирующему варианту.

``` C++
#include <boost/asio.hpp>
#include <boost/asio/co_spawn.hpp>
#include <boost/asio/detached.hpp>
#include <iostream>

using boost::asio::awaitable;
using boost::asio::co_spawn;
using boost::asio::detached;
using boost::asio::ip::tcp;
namespace this_coro = boost::asio::this_coro;

// Логика обработки одного клиента, оформленная как корутина
awaitable<void> session(tcp::socket socket) {
    try {
        char data[1024];
        for (;;) {
            // точка приостановки : асинхронное ожидание данных
            std::size_t n = co_await socket.async_read_some(
                boost::asio::buffer(data),
                boost::asio::use_awaitable);

            // ... выполнение продолжается здесь, когда данные получены ...

            // 2. асинхронная отправка данных
            co_await boost::asio::async_write(
                socket, 
                boost::asio::buffer(data, n), 
                boost::asio::use_awaitable);

            // ... выполнение продолжается здесь, когда данные отправлены ...
        }
    } catch (const std::exception& e) {
        // Исключения теперь работают естественным образом и являются частью состояния корутины
        std::cerr << "Session error: " << e.what() << std::endl;
    }
}

// Корутина, принимающая входящие соединения
awaitable<void> listener(short port) {
    auto executor = co_await this_coro::executor;
    tcp::acceptor acceptor(executor, {tcp::v4(), port});
    for (;;) {
        // асинхронное ожидание соединения
        tcp::socket socket = co_await acceptor.async_accept(boost::asio::use_awaitable);
        
        // ... выполнение продолжается здесь, когда соединение принято ...
        
        // Запускаем новую корутину для обработки клиента и не ждем ее завершения
        co_spawn(executor, session(std::move(socket)), detached);
    }
}

int main() {
    boost::asio::io_context io_context;

    // Запускаем слушающую корутину в контексте io_context
    co_spawn(io_context, listener(8080), detached);

    // Запускаем Event Loop
    io_context.run();
    return 0;
}
```


Визуально этот код выглядит также логично и линейно, как и блокирующая версия, но работает он полностью асинхронно на одном потоке.

Когда выполнение доходит до `co_await socket.async_read_some(...)` :

1. Вызывается функция `async_read_some`. Она, как и раньше, инициирует неблокирующую операцию чтения и **регистрирует в `io_context` внутренний обработчик**, связанный с дескриптором сокета.

2. Однако, вместо того, чтобы передавать сам обработчик, корутина возвращает специальный `awaitable` объект.

3. Рантайм вызывает у этого объекта метод, который приостанавливает текущую корутину. Это означает, что конечный автомат, сгенерированный компилятором, сохраняет своё текущее состояние и корутина `session` возвращает своё управление в `io_context`. Системный поток не блокируется и может обслуживать другие события.

4. Когда `epoll_wait` в цикле событий `io_context` сигнализирует о поступлении данных, `io_context` вызывает **тот самый** внутренний обработчик, зарегистрированный на шаге 1.

5. Этот обработчик в свою очередь, **возобновляет корутину** `session`. Её конечный автомат переходит в следующее состояние, и выполнение продолжается со строчки, следующей сразу за `co_await`.

Таким образом, вся сложная механика регистрации обработчиков, управления жизненным циклом и передачи состояния инкапсулируется компилятором и асинхронным фреймворком. Это позволяет сочетать производительность и масштабируемость событийной модели с простотой и читаемостью традиционного императивного кода.